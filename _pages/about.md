---
permalink: /
title: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<h1>Hi there!</h1>
<p style="text-align:justify">
My name is Hongyue Wang, I am a Ph.D. student in <a href="https://exertiongameslab.org/">Exertion games Lab</a> directed by Prof. <a href="https://www.florianfloydmueller.com/">Florian 'Floyd' Mueller</a> working on Human-Food Interaction (Associate Supervisor: Dr. <a href="https://samithaelvitigala.com/">Don Samitha Elvitigala</a> and Dr. <a href="https://nathansemertzidis.com/">Nathan Semertzidis</a>) at Monash University, Australia. I bring with me a rich background in HCI, XR, and AI development. My academic journey began at the University of Jinan, China, where I earned both my Bachelor's and Master's degrees in Computer Science and Technology, mentored by Prof. Zhiquan Feng.
</p>

<h1>My research interests</h1>
<p style="text-align:justify">
Human-Computer Interaction, Extended Reality, Interactive System, Deep Learning, Computer Vision, Generative AI, etc.
</p>

<h1>News</h1>
<ul>
<li><p style="text-align:justify"><b>05/2024:</b> I will attend <b>ACM CHI 2024</b>! See you in Hawaii!ðŸ˜Š</p></li>
<li><p style="text-align:justify"><b>03/2024:</b> Our two <b>ACM CHI 2024</b> late-breaking works have been accepted! Thanks to all the co-authors!</p></li>
<li><p style="text-align:justify"><b>09/2023:</b> I will join <b>Exertion Games Lab</b> at Monash University as a Ph.D. student. Another new journey strats!ðŸŽ‰</p></li>
<li><p style="text-align:justify"><b>05/2023:</b> I was awarded <b>Monash University-China Scholarship Council (CSC) Joint Scholarship</b>!ðŸ”¥</p></li>
<li><p style="text-align:justify"><strong>12/2022:</strong> I was awarded <b>National Scholarship for Chinese University Students</b> (Top 1%)!</p></li>
</ul>

<table style="border-collapse: collapse; border: none;">
<colgroup>
<col width="15%" />
<col width="85%" />
</colgroup>

<tbody style="border: none;">
<tr>
  <h1>Full Paper Publications</h1>
</tr>

<tr>
<td style="border: none;" markdown="span"><img src="/images/pic2eat2.jpg" width="250"></td>
<td markdown="span" style="border: none; text-align:justify"><a href="https://doi.org/10.1145/3613905.3651082" target="_blank">**pic2eat: Facilitating Social Ice-breaking through Collaborative Design of 3D Printed Appetizer**</a><br>**Hongyue Wang**, Jialin Deng, Aravind Mohan, Yinyi Li, Hao Peng, Linjia He, Don Samitha Elvitigala, Florian 'Floyd' Mueller<br>*CHI EA '24: Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems*<br><a href="../files/pic2eat.pdf" target="_blank">[paper]</a><a href="https://www.youtube.com/watch?v=d0GZcocqnV0" target="_blank">[video]</a></td>
</tr>

<tr>
<td style="border: none;"><img src="/images/penlab2.jpg" width="250"></td>
<td markdown="span" style="border: none; text-align:justify"><a href="https://dl.acm.org/doi/10.1145/3613905.3650789" target="_blank">**PenLab: Towards Understanding of Active Collaboration for Solid Geometry Teaching**</a><br>Dehui Kong, **Hongyue Wang**, Sijie Zhou, Hong Cui, Zhiquan Feng<br>*CHI EA '24: Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems*<br><a href="../files/penlab.pdf" target="_blank">[paper]</a><a href="https://www.youtube.com/watch?v=KzXrsdY8Xhw" target="_blank">[video]</a></td>
</tr>

<tr>
<td style="border: none;"><img src="/images/glove3.jpg" width="250"></td>
<td markdown="span" style="border: none; text-align:justify"><a href="https://link.springer.com/chapter/10.1007/978-981-19-4546-5_32">**Research on the Structure and Key Algorithms of Smart Gloves Oriented to Middle School Experimental Scene Perception**</a><br>**Hongyue Wang**, Xin Meng, and Zhiquan Feng\*<br>*ChineseCSCW '21: Computer Supported Cooperative Work and Social Computing*<br><a href="../files/Research on the Structure and Key Algorithms of Smart Gloves Oriented to Middle School Experimental Scene Perception.pdf" target="_blank">[paper]</a></td>
</tr>
</tbody>
</table>

<h1>Workshop Papers, Extended Abstracts, and Doctoral Consortium</h1>
<table style="border-collapse: collapse; border: none;">
<colgroup>
<col width="15%" />
<col width="85%" />
</colgroup>

<tbody style="border: none;">
<tr>
<td style="border: none;"><img src="/images/challenges.png" width="250"></td>
<td markdown="span" style="border: none; text-align:justify"><a href="https://www.sciencedirect.com/science/article/pii/S1071581923002069" target="_blank">**Grand challenges in human-food interaction**</a><br>Florian â€˜Floydâ€™ Mueller, Marianna Obrist, Ferran Altarriba Bertran, Neharika Makam, Soh Kim, Christopher Dawes, Patrizia Marti, Maurizio Mancini, Eleonora Ceccaldi, Nandini Pasumarthy, Sahej Claire, Kyung seo Jung, Jialin Deng, JÃ¼rgen Steimle, Nadejda Krasteva, Matti Schwalk, Harald Reiterer, **Hongyue Wang**, Yan Wang<br>*International Journal of Human-Computer Studies* 2024<br><a href="../files/grand challenges.pdf" target="_blank">[paper]</a></td>
</tr>

<tr>
<td style="border: none;"><img src="/images/2023_ijhci.png" width="250"></td>
<td markdown="span" style="border: none; text-align:justify"><a href="https://www.tandfonline.com/doi/abs/10.1080/10447318.2023.2227823" target="_blank">**MRLab: Virtual Reality Fusion Smart Laboratory Based on Multimodal Fusion**</a><br>**Hongyue Wang**, Zhiquan Feng\*, Xiaohui Yang, Liran Zhou, Jinglan Tian, and Qingbei Guo<br>*International Journal of Human-Computer Interaction* 2023<br><a href="../files/MRLab.pdf" target="_blank">[paper]</a></td>
</tr>

<tr>
<td style="border: none;"><img src="/images/2023_robot.png" width="250"></td>
<td markdown="span" style="border: none; text-align:justify"><a href="https://www.tandfonline.com/doi/abs/10.1080/10447318.2023.2247606?journalCode=hihc20" target="_blank">**MIUIC: A Human-Computer Collaborative Multimodal Intention-Understanding Algorithm Incorporating Comfort Analysis**</a><br>Liran Zhou, Zhiquan Feng\*, **Hongyue Wang**, and Qingbei Guo<br>*International Journal of Human-Computer Interaction* 2023<br><a href="../files/MIUIC.pdf" target="_blank">[paper]</a></td>
</tr>

<tr>
<td style="border: none;"><img src="/images/glove2.png" width="250"></td>
<td markdown="span" style="border: none; text-align:justify"><a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/admt.202200549">**Multimodal Information Perception and Understanding: Application of Smart Glove in Virtual-Reality Fusion Chemistry Experiment Platform**</a><br>**Hongyue Wang**, Zhiquan Feng\*, and Xin Meng<br>*Advanced Materials Technologies* 2023<br><a href="../files/Multimodal Information Perception and Understandingï¼šApplication of Smart Glove in Virtual-Reality Fusion Chemistry Experiment Platform.pdf" target="_blank">[paper]</a><a href="https://youtu.be/8btwYEO1cd4" target="_blank">[video]</a></td>
</tr>

<tr>
<td style="border: none;"><img src="/images/glove1.png" width="250"></td>
<td markdown="span" style="border: none; text-align:justify"><a href="https://www.hindawi.com/journals/cin/2022/3545850/" target="_blank">**MFA: A Smart Glove with Multimodal Intent Sensing Capability**</a><br>**Hongyue Wang**, Zhiquan Feng\*, Jinglan Tian, and Xue Fan<br>*Computational Intelligence and Neuroscience* 2022<br><a href="../files/MFA.pdf" target="_blank">[paper]</a></td>
</tr>



</tbody>
</table>
